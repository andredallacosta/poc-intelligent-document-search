"""Add file_upload and document_processing_job tables for ADR-002

Revision ID: 1acc2f7e09b0
Revises: f26167aa9196
Create Date: 2025-09-21 11:34:24.272859

"""

from typing import Sequence, Union

import sqlalchemy as sa

from alembic import op

# revision identifiers, used by Alembic.
revision: str = "1acc2f7e09b0"
down_revision: Union[str, Sequence[str], None] = "f26167aa9196"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "document_processing_job",
        sa.Column("id", sa.UUID(), nullable=False),
        sa.Column("document_id", sa.UUID(), nullable=False),
        sa.Column("upload_id", sa.UUID(), nullable=False),
        sa.Column("status", sa.String(length=50), nullable=False),
        sa.Column("current_step", sa.String(length=500), nullable=False),
        sa.Column("progress", sa.Integer(), nullable=False),
        sa.Column("chunks_processed", sa.Integer(), nullable=False),
        sa.Column("total_chunks", sa.Integer(), nullable=False),
        sa.Column("processing_time_seconds", sa.Integer(), nullable=False),
        sa.Column("s3_file_deleted", sa.Boolean(), nullable=True),
        sa.Column("duplicate_of", sa.UUID(), nullable=True),
        sa.Column("content_hash_algorithm", sa.String(length=20), nullable=True),
        sa.Column("content_hash_value", sa.String(length=64), nullable=True),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column("meta_data", sa.JSON(), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=True,
        ),
        sa.Column("started_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("completed_at", sa.DateTime(timezone=True), nullable=True),
        sa.CheckConstraint(
            "status IN ('uploaded', 'extracting', 'checking_duplicates', 'chunking', 'embedding', 'completed', 'failed', 'duplicate')",
            name="check_valid_status",
        ),
        sa.CheckConstraint(
            "chunks_processed >= 0", name="check_chunks_processed_positive"
        ),
        sa.CheckConstraint(
            "processing_time_seconds >= 0", name="check_processing_time_positive"
        ),
        sa.CheckConstraint(
            "progress >= 0 AND progress <= 100", name="check_progress_range"
        ),
        sa.CheckConstraint("total_chunks >= 0", name="check_total_chunks_positive"),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        "idx_processing_job_created",
        "document_processing_job",
        ["created_at"],
        unique=False,
    )
    op.create_index(
        "idx_processing_job_document_id",
        "document_processing_job",
        ["document_id"],
        unique=False,
    )
    op.create_index(
        "idx_processing_job_status", "document_processing_job", ["status"], unique=False
    )
    op.create_index(
        "idx_processing_job_upload_id",
        "document_processing_job",
        ["upload_id"],
        unique=False,
    )
    op.create_table(
        "file_upload",
        sa.Column("id", sa.UUID(), nullable=False),
        sa.Column("document_id", sa.UUID(), nullable=False),
        sa.Column("filename", sa.String(length=255), nullable=False),
        sa.Column("file_size", sa.Integer(), nullable=False),
        sa.Column("content_type", sa.String(length=100), nullable=False),
        sa.Column("s3_bucket", sa.String(length=63), nullable=True),
        sa.Column("s3_key", sa.String(length=1024), nullable=True),
        sa.Column("s3_region", sa.String(length=50), nullable=True),
        sa.Column("upload_url", sa.Text(), nullable=True),
        sa.Column("expires_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("uploaded_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=True,
        ),
        sa.CheckConstraint("file_size > 0", name="check_file_size_positive"),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        "idx_file_upload_created", "file_upload", ["created_at"], unique=False
    )
    op.create_index(
        "idx_file_upload_document_id", "file_upload", ["document_id"], unique=False
    )
    op.drop_index(op.f("idx_documento_metadata_source"), table_name="documento")
    op.create_index(
        "idx_documento_metadata_source",
        "documento",
        [sa.literal_column("json_extract_path_text(meta_data, 'source')")],
        unique=False,
    )
    op.drop_index(op.f("idx_documento_source_unique"), table_name="documento")
    op.create_index(
        "idx_documento_source_unique",
        "documento",
        [sa.literal_column("json_extract_path_text(meta_data, 'source')")],
        unique=True,
    )
    op.drop_index(
        op.f("idx_documento_embedding_vector"),
        table_name="documento_embedding",
        postgresql_ops={"embedding": "vector_cosine_ops"},
        postgresql_with={"lists": "100"},
        postgresql_using="ivfflat",
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_index(
        op.f("idx_documento_embedding_vector"),
        "documento_embedding",
        ["embedding"],
        unique=False,
        postgresql_ops={"embedding": "vector_cosine_ops"},
        postgresql_with={"lists": "100"},
        postgresql_using="ivfflat",
    )
    op.drop_index("idx_documento_source_unique", table_name="documento")
    op.create_index(
        op.f("idx_documento_source_unique"),
        "documento",
        [
            sa.literal_column(
                "json_extract_path_text(meta_data, VARIADIC ARRAY['source'::text])"
            )
        ],
        unique=True,
    )
    op.drop_index("idx_documento_metadata_source", table_name="documento")
    op.create_index(
        op.f("idx_documento_metadata_source"),
        "documento",
        [
            sa.literal_column(
                "json_extract_path_text(meta_data, VARIADIC ARRAY['source'::text])"
            )
        ],
        unique=False,
    )
    op.drop_index("idx_file_upload_document_id", table_name="file_upload")
    op.drop_index("idx_file_upload_created", table_name="file_upload")
    op.drop_table("file_upload")
    op.drop_index("idx_processing_job_upload_id", table_name="document_processing_job")
    op.drop_index("idx_processing_job_status", table_name="document_processing_job")
    op.drop_index(
        "idx_processing_job_document_id", table_name="document_processing_job"
    )
    op.drop_index("idx_processing_job_created", table_name="document_processing_job")
    op.drop_table("document_processing_job")
    # ### end Alembic commands ###
