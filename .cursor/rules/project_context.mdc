---
alwaysApply: true
---

# POC - Vector Database para Documentos

## ğŸ¯ OBJETIVO

Criar uma POC para ingestÃ£o e indexaÃ§Ã£o de documentos diversos (PDF, DOCX, links) transformando-os em trechos Ãºteis e buscÃ¡veis semanticamente para uso como contexto de IA.

## ğŸ“‹ ARQUITETURA DEFINIDA

### Pipeline de IngestÃ£o (Python)

- **Parsing**: unstructured + python-docx + trafilatura  
- **Chunking**: LangChain (chunking semÃ¢ntico)
- **Embeddings**: OpenAI text-embedding-3-small
- **Vector DB**: Chroma (local, SQLite, zero config)

### API de Consumo (Node.js - futuro)

- Consultas ao Chroma + chamadas para LLM

## ğŸ—‚ ESTRUTURA DO PROJETO

```
/home/andre/Projects/poc-intelligent-document-search/
â”œâ”€â”€ documents/           # PDFs, DOCs de entrada
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py       # Pipeline de ingestÃ£o
â”‚   â”œâ”€â”€ chunker.py      # LÃ³gica de chunking
â”‚   â”œâ”€â”€ embedder.py     # GeraÃ§Ã£o de embeddings
â”‚   â””â”€â”€ query.py        # Busca e retrieval
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chroma_db/      # Base vetorial local
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”„ FLUXO DE INGESTÃƒO

### 1. Upload + Parsing

**Entrada**: arquivos .pdf, .docx, .doc, links
**SaÃ­da**:

```json
{
  "text": "Solicito a Vossa ExcelÃªncia a emissÃ£o do documento XYZ...",
  "metadata": {
    "source": "manual_redacao.pdf", 
    "page": 4,
    "tipo_documento": "manual",
    "assunto": "estrutura do ofÃ­cio"
  }
}
```

### 2. Chunking

- **Tamanho**: 300-500 tokens por chunk
- **EstratÃ©gia**: SemÃ¢ntico (por parÃ¡grafo/sentenÃ§a lÃ³gica)
- **Overlapping**: 50 tokens de sobreposiÃ§Ã£o
- **Especial**: Para docs jurÃ­dicos, considerar quebras por artigo/inciso

### 3. Embeddings

- **Modelo**: OpenAI text-embedding-3-small
- **SaÃ­da**: Vetor + metadados para cada chunk

### 4. Armazenamento

- **Chroma**: Banco vetorial local (SQLite)
- **Metadados**: Filtros por tipo, pÃ¡gina, documento, etc.

## ğŸ› DECISÃ•ES TÃ‰CNICAS

### Por que Python para ingestÃ£o?

- Melhores bibliotecas para processamento de documentos
- LangChain maduro para chunking semÃ¢ntico
- Ecossistema de IA mais robusto

### Por que Chroma?

- Zero setup (pip install chromadb)
- PersistÃªncia local em arquivo
- FÃ¡cil migraÃ§Ã£o futura para Weaviate/Qdrant
- Suporte nativo a filtros por metadata

### Por que OpenAI Embeddings?

- Custo x performance equilibrado
- API estÃ¡vel e documentada
- Compatibilidade com diversos vector DBs

## ğŸš§ PRÃ“XIMOS PASSOS

1. ~~Implementar pipeline bÃ¡sico de ingestÃ£o~~ âœ… CONCLUÃDO
2. Testar com documentos reais do usuÃ¡rio
3. ~~Implementar busca e retrieval~~ âœ… CONCLUÃDO  
4. Avaliar qualidade das respostas
5. API Node.js para consumo

## ğŸ¯ PRONTO PARA TESTE

O pipeline estÃ¡ implementado e funcional. Para testar:

1. Criar arquivo `.env` com `OPENAI_API_KEY`
2. Instalar dependÃªncias: `pip install -r requirements.txt`
3. Colocar documentos na pasta `documents/`
4. Executar: `python src/ingest.py` (ingestÃ£o)
5. Executar: `python src/query.py` (busca)

## ğŸ’¡ NOTAS IMPORTANTES

- UsuÃ¡rio Ã© iniciante em vector databases
- Chunking semÃ¢ntico serÃ¡ implementado conforme necessÃ¡rio
- LLM de consumo serÃ¡ definido depois (provavelmente GPT-4o-mini)
- AvaliaÃ§Ã£o de qualidade serÃ¡ implementada posteriormente

## ğŸš« REGRAS DE DESENVOLVIMENTO

- **NÃƒO adicionar comentÃ¡rios no cÃ³digo**
- **NÃƒO criar ou atualizar README automaticamente**
- **NÃƒO adicionar explicaÃ§Ãµes em arquivos**
- **Gerar cÃ³digo e arquivos SOMENTE quando o usuÃ¡rio autorizar explicitamente**
- **NÃƒO concordar sempre com o usuÃ¡rio, sugerir mudanÃ§as e alertar sobre possÃ­veis problemas**
